---
title: word2vec
date: '2022-11-15'
tags: ['Python', 'Deep Learning']
draft: false
summary: '学习word2vec'
authors: ['default']
---

# 向量空间中单词表示的有效估计

**摘要**

我们提出了两种新颖的模型架构, ⽤于从⾮常⼤的数据集中计算单词的连续向量表⽰。这些表⽰的质量在 单词相似度任务中进行测量, 并将结果与以前基于不同类型神经⽹络的最佳性能技术进行⽐较。我们观察 到以更低的计算成本⼤幅提⾼了准确性, 即从 16 亿个单词数据集中学习⾼质量的单词向量只需不到⼀天 的时间。此外, 我们表明这些向量在我们的测试集上提供了最先进的性能, ⽤于测量句法和语义词的相似性。

## 1.介绍

许多当前的 NLP 系统和技术将单词视为原⼦单元 - 单词之间没有相似性的概念, 因为它们表⽰为词汇表中的索引。这种选择有⼏ 个很好的理由 - 简单性、稳健性以及观察到在⼤量数据上训练的简单模型优于在较少数据上训练的复杂系统。⼀个例⼦是⽤于统 计语⾔建模的流行 N-gram 模型 - 今天, 可以在⼏乎所有可⽤数据（数万亿个单词 [3]）上训练 N-gram。

然⽽, 简单的技术在许多任务中都处于其极限。例如, ⽤于⾃动语⾳识别的相关域内数据量是有限的性能通常由⾼质量转录语⾳数据的⼤⼩（通常只有数百万字）决定。在机器翻译中, 许多语⾔的现有语料库只包含⼏⼗亿或更少的单词。因此, 在某些情况下, 简单地扩⼤基本技术不会带来任何重⼤进展, 我们必须专注于更先进的技术。

随着近年来机器学习技术的进步, 可以在更⼤的数据集上训练更复杂的模型, 并且它们通常优于简单模型。可能最成功的概念是使 ⽤词的分布式表⽰[10]。例如, 基于神经⽹络的语⾔模型明显优于 N-gram 模型 [1, 27, 17]。

### 1.1论文目标

本⽂的主要⽬标是介绍可⽤于从包含数⼗亿单词和词汇表中数百万单词的庞⼤数据集中学习⾼质量单词向量的技术。据我们所知, 之前提出的架构都没有成功地训练过更多超过⼏亿个单词, 单词向量的适度维度在 50 到 100 之间。

我们使⽤最近提出的技术来测量结果向量表⽰的质量, 期望相似的词不仅趋于彼此接近, ⽽且词可以具有多个相似度[20]。早先在屈折语⾔的上下⽂中已经观察到这⼀点例如, 名词可以有多个词尾, 如果我们在原始向量空间的⼦空间中搜索相似词, 就有可能找到具有相似词尾的词 [13 , 14】

有点令⼈惊讶的是, ⼈们发现单词表⽰的相似性超出了简单的句法规则。使⽤词偏移技术, 其中对词向量执行简单的代数运算, 例如, 向量（“King”）-向量（“Man”）+向量（“Woman”）导致向量为最接近单词 Queen [20] 的向量表⽰。

在本⽂中, 我们尝试通过开发新的模型架构来最⼤限度地提⾼这些向量运算的准确性, 以保持单词之间的线性规律。我们设计了⼀ 个新的综合测试集来测量句法和语义规则1 , 并表明可以以⾼精度学习许多这样的规则。此外, 我们讨论了训练时间和准确性如何取 决于词向量的维度和训练数据的数量。

### 1.2 前期工作

将单词表⽰为连续向量具有悠久的历史 [10, 26, 8]。在 [1] 中提出了⼀种⾮常流行的⽤于估计神经⽹络语⾔模型 (NNLM) 的模型 架构, 其中使⽤具有线性投影层和⾮线性隐藏层的前馈神经⽹络来联合学习词向量表⽰和统计语⾔模型。这项⼯作已被许多其他⼈ 效仿

[13, 14] 中提出了另⼀个有趣的 NNLM 架构, 其中⾸先使⽤具有单个隐藏层的神经⽹络来学习词向量。然后使⽤词向量来训练 NNLM。因此, 即使没有构建完整的 NNLM, 也可以学习词向量。在这项⼯作中, 我们直接扩展了这个架构, 并只关注使⽤简单模型学 习词向量的第⼀步。

后来表明, 词向量可⽤于显着改进和简化许多 NLP 应⽤程序 [4, 5, 29]。词向量本⾝的估计是使⽤不同的模型架构进行的, 并在各种 语料库上进行训练 [4, 29, 23, 19, 9], 并且⼀些得到的词向量可⽤于未来的研究和⽐较2 。然⽽, 据我们所知, 这些架构的训练计算 成本明显⾼于 [13] 中提出的架构, 除了使⽤对⻆权重矩阵的某些版本的对数双线性模型 [23]。

## 2. 模型架构

提出了许多不同类型的模型来估计单词的连续表⽰, 包括众所周知的潜在语义分析（LSA）和潜在狄利克雷分配（LDA）。

在本⽂中, 我们专注于神经⽹络学习的单词的分布式表⽰, 因为之前已经证明它们在保持单词之间的线性规律性⽅⾯的表现明显优 于 LSA [20, 31]；此外, LDA 在⼤型数据集上的计算成本⾮常⾼。

与 [18] 类似, 为了⽐较不同的模型架构, 我们⾸先将模型的计算复杂度定义为完全训练模型需要访问的参数数量。接下来, 我们将 尝试最⼤化准确性, 同时最⼩化计算复杂度。

对于以下所有模型, 训练复杂度为

$$
O = E × T × Q,
$$

其中 E 是训练 epoch （时间步）的数量, T 是训练集中的单词数, Q 为每个模型架构进⼀步定义。常⻅的选择是 E = 3 － 50 和 T ⾼达 10 亿。所有模型都使⽤随机梯度下降和反向传播[26]进行训练。

### 2.1 前馈神经网络语言模型（NNLM）

概率前馈神经⽹络语⾔模型已在 [1] 中提出。它由输⼊层、投影层、隐藏层和输出层组成。在输⼊层, N 个先前的单词 使⽤ 1-of-V 编码进行编码, 其中 V 是词汇表的⼤⼩。然后使⽤共享投影矩阵将输⼊层投影到维度为 N × D 的投影 层 P。由于在任何给定时间只有 N 个输⼊处于活动状态, 因此投影层的组合是⼀种相对便宜的操作。

NNLM 架构对于投影和隐藏层之间的计算变得复杂, 因为投影层中的值是密集的。对于 N = 10 的常⻅选择, 投影层 (P) 的⼤⼩可能为 500 到 2000, ⽽隐藏层⼤⼩ H 通常为 500 到 1000 个单位。此外, 隐藏层⽤于计算词汇表中所有 单词的概率分布, 从⽽产⽣⼀个维数为 V 的输出层。因此, 每个训练⽰例的计算复杂度为

$$
Q = N × D + N × D × H + H × V,
$$

其中主导项是 H × V 。但是, 为了避免这种情况, 提出了⼏种实际的解决⽅案；要么使⽤ softmax [25, 23, 18] 的分 层版本, 要么通过使⽤在训练期间未归⼀化的模型来完全避免归⼀化模型 [4, 9]。使⽤词汇表的⼆叉树表⽰, 需要评 估的输出单元的数量可以下降到log2(V ) 左右。因此, ⼤部分复杂性是由 N × D × H 项引起的。

在我们的模型中, 我们使⽤分层 softmax, 其中词汇表表⽰为 Huffman ⼆叉树。这遵循了先前的观察, 即单词的频率 对于在神经⽹络语⾔模型中获得类很有效 [16]。霍夫曼树将短⼆进制代码分配给频繁词, 这进⼀步减少了需要评估 的输出单元的数量：虽然平衡⼆叉树需要评估log2(V ) 输出, 但基于霍夫曼树的分层 softmax 只需要⼤约log2 （⼀ 元困惑度（V））。例如, 当词汇量为 100 万个单词时, 这会导致评估速度提⾼⼤约两倍。虽然这对于神经⽹络 LM 来 说并不是⾄关重要的加速, 因为计算瓶颈在 N×D×H 项中, 但我们稍后将提出没有隐藏层的架构, 因此严重依赖于 softmax 归⼀化的效率。

### 2.2 循环神经网络语言模型（RNNLM）

已经提出了基于循环神经⽹络的语⾔模型来克服前馈 NNLM 的某些限制, 例如需要指定上下⽂长度（模型 N 的阶 数）, 并且因为理论上 RNN 可以⽐浅层神经⽹络有效地表⽰更复杂的模式⽹络 [15, 2]。 RNN模型没有投影层；只 有输⼊层、隐藏层和输出层。这种模型的特别之处在于使⽤延时连接将隐藏层连接到⾃⾝的循环矩阵。这允许循环模 型形成某种短期记忆, 因为过去的信息可以由隐藏层状态表⽰, 该隐藏层状态根据当前输⼊和前⼀个时间步的隐藏 层状态进行更新。

RNN 模型的每个训练样例的复杂度为

$$
Q = H × H + H × V,
$$

其中单词表⽰ D 与隐藏层 H 具有相同的维度。同样, 通过使⽤分层 softmax, 可以将 H × V 项有效地简化为 H × log2(V )。⼤部分复杂度来⾃ H × H。

### 2.3 神经网络的并行训练

为了在庞⼤的数据集上训练模型, 我们在⼀个名为 DistBelief [6] 的⼤规模分布式框架之上实现了⼏个模型, 包括前馈 NNLM 和本⽂提出的新模型。该框架允许我们并行运行同⼀模型的多个副本, 每个副本通过保留所有参数的集中式服务 器同步其梯度更新。对于这种并行训练, 我们使⽤⼩批量异步梯度下降和称为 Adagrad [7] 的⾃适应学习率过程。在此框 架下, 通常使⽤⼀百个或更多模型副本, 每个副本在数据中⼼的不同机器上使⽤许多 CPU 内核。

## 3. 新的对数线性模型（线性模型+ softmax）

在本节中, 我们提出了两种新的模型架构, ⽤于学习单词的分布式表⽰, 以尽量减少计算复杂度。上⼀节的主要观察是, ⼤部 分复杂性是由模型中的⾮线性隐藏层引起的。虽然这就是神经⽹络如此吸引⼈的原因, 但我们决定探索更简单的模型, 这 些模型可能⽆法像神经⽹络那样精确地表⽰数据, 但可以有效地在更多数据上进行训练。

新架构直接遵循我们早期⼯作 [13, 14] 中提出的架构, 其中发现神经⽹络语⾔模型可以通过两个步骤成功训练：⾸先, 使 ⽤简单模型学习连续词向量, 然后使⽤ N- gram NNLM 在这些分布式单词表⽰之上进行训练。虽然后来有⼤量⼯作集中 在学习词向量上, 但我们认为 [13] 中提出的⽅法是最简单的⽅法。

### 3.1连续词袋模型

第⼀个提出的架构类似于前馈 NNLM, 其中去除了⾮线性隐藏层, 所有单词共享投影层（不仅仅是投影矩阵）；因此, 所有 单词都被投影到相同的位置（它们的向量被平均）。我们称这种架构为词袋模型, 因为历史中的词顺序不会影响投影。

此外, 我们还使⽤来⾃未来的词语；我们在下⼀节介绍的任务中获得了最佳性能, ⽅法是构建⼀个对数线性分类器, 输⼊有 四个未来词和四个历史词, 其中训练标准是正确分类当前（中间）词。

训练复杂度为

$$
Q = N × D + D × log2(V )。 (4)
$$

我们将此模型进⼀步表⽰为 CBOW, 与标准的词袋模型不同, 它使⽤上下⽂的连续分布式表⽰。模型架构如图 1 所⽰。请注 意, 输⼊层和投影层之间的权重矩阵以与 NNLM 中相同的⽅式为所有单词位置共享。

### 3. 2 连续Skip-gram模型

第二种架构类似于 CBOW, 但它不是根据上下文预测当前单词, 而是尝试根据同一句子中的另一个单词最大化对单词的分类。

更准确地说, 我们使用每个当前单词作为具有连续投影层的对数线性分类器的输入, 并预测当前单词前后一定范围内的单词。我们发现增加范围可以提高结果词向量的质量, 但也会增加计算复杂度。由于距离较远的词通常与当前词的相关性低于与当前词的相关性, 因此我们通过在训练示例中从这些词中抽取较少的样本来给予较远的词更少的权重。

这种架构的训练复杂度为：

$$
Q = C × (D + D × log(2V))
$$

其中, C是单词的最大距离。

## 1. 连续词袋模型

### 1.1 one-word context（上下文一个词）

从词袋模型的最简单版本开始。我们假设每个上下文只考虑一个单词, 这意味着给定模型一个上下文词去预测一个目标词, 就像bigram模型。

图1显示了简化了上下文定义下的网络模型。规定, 词汇量大小为V, 隐含层大小为N。相邻层的单元是全连接的, 输入是一个one-hot 向量, 这意味着给的一个上下文单词, ${x_1,x_2 ... x_v}$中只有一个1。

![image-20221010164521603](/static/images/202211/image-20221010164521603.png)

输入层和输出层之间的权重可以表示成一个V×N的矩阵W。W的每一行是一个N维的向量$V_W$。

通常, W的行$i$ 是$v_w^T$。给定一个上下文单词, 设$x_k = 1$, 则

$$
h = W^Tx = W^T_{(k,.)}:=v_{w_I}^T               \quad (1)
$$

也就是将 W 的第 k 行复制到 h, $v_{w_I}$是输入单词$w_I$的向量表示。这暗示了隐含层的激活函数就是一个简单的线性函数。

从隐含层到输出层, 有一个不同的权重矩阵, $W^{'}={w_{ij}^{'}}$, 是一个$N × V$的矩阵。使用这些权重, 我们可以为词汇表中每一个单词计算一个分数 $u_j$,

$$
u_j = v_{w_j}^{'T}h \quad (2)
$$

其中, $v_{w_j}^{'}$是$W^{'}$的第 $j$ 列, 即一个 $ N * 1$的矩阵。

然后可以使用softmax, 一个对数线性分类模型, 去得到词的后验分布, 是一个多项分布。

$$
p(w_j|w_I) = y_j = \frac{exp(u_j)}{\sum_{j^{'}=1}^{V}exp(u_{j^{'}})} \quad(3)
$$

其中, $y_j$是输出层单元 j 的输出。把$(1) 和(2)$带入 $(3)$得到

$$
p(w_j|w_I) = \frac{exp(v_{w_j}^{'T}v_{w_I})}{\sum_{j^{'}=1}^{V}exp(v_{w_{j^{'}}}^{'}v_{w_I})}\quad(4)
$$

注意, $v_w$和$v_w^{'}$是单词$w$的两种表示形式。$v_w$来自W的行向量, 是输入--隐含层的矩阵。而$v_w^{'}$来自$W^{'}$的列, 是隐含层--输出层的矩阵。在随后的分析中, 称$v_w$为“输出向量”, 而$v_w^{'}$为“输出向量”, 它们都是单词 $w$的向量。

**更新隐含层---》输出层权重的方程**

训练目标是最大化 （4）式, 即最大化给定上下文单词 $w_I$下输出一个正确的输出 $w_O$的条件概率。

$$
maxp(w_O|w_I)=maxy_j*\quad(5)\\=maxlogy_j*\quad(6)\\=u_j*-log\sum_{j^{'}=1}^{V}exp(u_{j^{'}}):=-E\quad(7)
$$

其中, $E=-logp(w_O|w_I)$是我们的代价函数（我们想要最小化E）, $j*$是输出层实际输出的索引。注意：这个损失函数可以理解为两个概率分布之间的交叉熵测量的特殊情况。

下面推导隐含层和输出层之间的权重更新公式。

$$
\frac{\partial E}{\partial u_j}=y_j-t_j:=e_j\quad(8)
$$

$t_j=1$只在第 j 单元是实际输出单词, 否则为0. 注意, 这个导数就是输出层的预测误差ej。

接下来对$w_{ij}^{'}$求导获得隐含层---》输出层权重的梯度。

$$
\frac{\partial E}{\partial w_{ij}^{'}}=\frac{\partial E}{\partial u_j}*\frac{\partial u_j}{\partial w_{ij}^{'}}=e_j*h_i\quad(9)
$$

因此, 使用随机梯度下降, 我们得到隐含层--》输出层的权重更新方程为

$$
w_{ij}^{'(new)}=w_{ij}^{'(old)}-\eta*e_j*h_j\quad(10)
$$

或者

$$
v_{w_j}^{'(new)}=v_{w_j}^{'(old)}-\eta*e_j*h\quad for j=1,2,...,V.\quad(11)
$$

其中, $\eta$是学习率, $e_j = y_j-t_j$, $h_i$是隐含层的第 i 个单元；$v_{w_j}^{'}$是输出向量$w_j$。

注意, 这个更新方程意味着我们必须遍查词汇表中每一个可能的单词, 检查其输出概率$y_j$, 并与它的预期输出$t_j$进行比较(要么0, 要么1)。如果 $y_j>t_j$, 就减少隐含向量h的一部分, 让$v_{w_j}^{'}$距离$v_{w_I}$更远；如果 $y_j<t_j$, 就增加h的一部分, 让$v_{w_j}^{'}$距离$v_{w_I}$更近；如果yj非常接近tj, 那么根据更新方程, 权值变化很小。

**更新输入--》隐含层的权重的方程**

用E对隐含层的输出求偏导

$$
\frac{\partial E}{\partial h_i}=\sum_{j=1}^{V}\frac{\partial E}{\partial u_j}*\frac{\partial u_j}{\partial h_i}=\sum_{j=1}^{V}e_j*w_{ij}^{'}:=EH_i\quad(12)
$$

其中, $h_i$是隐含层的第 i 个单元的输出；$u_j$在（2）中被定义, 输出层的第j个单元的净输入；

$e_j=y_j-t_j$是输出层的第j个单词的预测误差。EH, 一个N维的向量, 是词汇表中所有单词的输出向量之和, 并以他们的预测误差加权。

接下来要对W求导。展开（1）式可得

$$
h_i=\sum_{k=1}^{V}x_k*w_{ki}\quad(13)
$$

现在可以对W的每一个元素求导,

$$
\frac{\partial E}{\partial w_{ki}}=\frac{\partial E}{\partial h_i}*\frac{\partial h_i}{\partial w_{ki}}=EH_i*x_k\quad(14)
$$

这等价于x和EH的张量积,

$$
\frac{\partial E}{\partial W}=x \otimes EH=xEH^T\quad(15)
$$

从而得到一个 $V * N$的矩阵, 因为x只有一个分量是非0 的, $\frac{\partial E}{\partial W}$只有一行是非0 的, 并且那一行的值为$EH^T$, 一个N维的向量。我们得到W的更新公式为,

$$
v_{w_I}^{new}=v_{w_I}^{old}-\eta EH^T\quad(16)
$$

其中, $v_{w_I}$是W的一行, 是唯一上下文单词的“输入向量”, 并且是W中唯一导数非零的行。在这个迭代之后, W的所有其他行将保持不变, 因为它们的导数为零。

直观地说, 由于向量EH是词汇表中所有单词的输出向量之和, 其预测误差$e_j=y_j-t_j$, 因此我们可以将 (16) 理解为将词汇表中每个输出向量的一部分加到上下文单词的输入向量上。如果在输出层, 一个词$w_j$作为输出词的概率被高估($y_j > t_j$), 那么上下文词$w_I$的输入向量将趋向于远离$w_j$的输出向量;相反, 如果$w_j$是输出字的概率被低估($y_j < t_j$), 则输入向量$w_I$将趋向于向输出向量$w_j$靠拢;

### 1.2 多词上下文

图2显示了具有多词上下文的CBOW模型。在计算隐层输出时, CBOW模型没有直接复制输入上下文词的输入向量, 而是取输入上下文词的向量的平均值, 并且使用输入层--》隐含层权重矩阵和平均向量的乘积作为输出。

$$
h = \frac{1}{C}W^T(x_1,x_2,...,x_C) \quad(17)\\= \frac{1}{C}(v_{w_1}+v_{w_2}+...+v_{w_C})^T\quad(18)
$$

C是上下文中的单词数, w1, · · · , wC是上下文中的单词数, $v_w$是单词w的输入向量。

损失函数为：

$$
E =-logp(w_O|w_{I,1},...,w_{I,C})\quad(19)\\=-u_{j^*}+log\sum_{j^{'}}^{V}exp(u_{j^{'}})\quad(20)\\=-v_{w_O}^{'T}*h+log\sum_{j^{'}}^{V}exp(v_{w_j}^{'T}*h)\quad(21)
$$

上式和（7）式一样, 单词上下文模型的目标函数, 除了 在（18）和（1）中定义的**h** 不一样。

![image-20221011095352742](/static/images/202211/image-20221011095352742.png)

隐含层--》输出层的更新方程与单子上下文模型的（11）一样。

$$
v_{w_j}^{'(new)}=v_{w_j}^{'(old)}-\eta*e_j*h\quad for j=1,2,...,V.\quad(22)
$$

注意, 我们需要将此应用于每个训练实例的hidden→output权重矩阵的每个元素。

输入→隐藏权重的更新方程类似于 (16) , 不同的是, 现在我们需要对上下文中的每个单词$w_{I,C}$应用以下方程:

$$
v_{w_{I,c}}^{new}=v_{w_{I,c}}^{old}-\frac{1}{C}\eta EH^T\quad for c=1,2,...,C.\quad(23)
$$

$v_{w_{I,c}}$是输入上下文中第 c 个单词的输入向量；$\eta$是一个正的学习率；$EH=\frac{\partial E}{\partial h_i}$从（12）中得出。这个更新方程的直观理解与（16）一样。

## 2.Skip-Gram Model

![image-20221011101529571](/static/images/202211/image-20221011101529571.png)

仍然用$v_{w_I}$代表输入层的唯一单词, 所以对于隐含层的输出 **h** 有跟（1）相同的定义, 即 **h**就是复制了输入--》隐含层的权重矩阵 **W** 的一行。

$$
h = W^T_{(k,.)}:=v_{w_I}^T               \quad (24)
$$

在输出层, 输出 **C**个多项分布。每一个输出使用同一个隐含层--》输出层的权重矩阵。

$$
p(w_{c,j}=w_{O,c}|w_I)=y_{c,j}=\frac{exp(u_{c,j})}{\sum_{j^{'}=1}^{V}exp(u_{j^{'}})}\quad(25)
$$

其中, $w_{c,j}$是第 j 个单词在第 c 个输出中。$w_{c,j}$是实际应该第c 个输出的词。$y_{c,j}$是第 c 个输出的 第 j 位。

$u_{c,j}$是净输入, 因为共享权重, 所以

$$
u_{c,j} =u_j= v_{w_j}^{'T}h \quad for c=1,2,...,C.\quad (26)
$$

其中, $v_{w_j}^{'}$是词汇表中第 j 个单词$w_j$的输出向量, 也是隐含层--》输出层权重矩阵$W^{'}$的一列。

参数方程的推导与单词上下文模型没有太大的区别。

$$
E=-logp(w_{O,1},w_{O,2},...w_{O,c}|w_I)\quad(27)\\=-log\prod_{c=1}^{C}\frac{exp(u_{c,j_c^*})}{\sum_{j^{'}=1}^{V}exp(u_{j^{'}})}\quad(28)\\=-\sum_{c=1}^{C}u_{j_c^*}+C*log\sum_{j^{'}=1}^{V}exp(u_{j^{'}})\quad (29)
$$

其中, $j_c^*$是实际输出的第c个上下文单词。

对 输出层的输入求偏导得到

$$
\frac{\partial E}{\partial u_{c,j}}=y_{c,j}-t_{c,j}:=e_{c,j}\quad (30)
$$

即单元上的预测误差, 与（8）相同。为了简单, 定义 V 维的向量 $EI=\{{EI_1,...,EI_V}\}$作为所有误差的和

$$
EI_j=\sum_{c=1}^{C}e_{c,j}\quad (31)
$$

对隐含层--》输出层的权重$W^{'}$求导得到

$$
\frac{\partial E}{\partial w_{ij}^{'}}=\sum_{c=1}^{C}\frac{\partial E}{\partial u_{c,j}}*\frac{\partial u_{c,j}}{\partial w_{ij}^{'}}=EI_j*h_i\quad(32)
$$

然后获得隐含层--》输出层的权重的更新方程：

$$
w_{ij}^{'(new)}=w_{ij}^{'(old)}-\eta*EI_j*h_i\quad(33)
$$

或者

$$
v_{w_j}^{'(new)}=v_{w_j}^{'(old)}-\eta*EI_j*h\quad for j=1,2,...,V.\quad(34)
$$

输入层--》隐含层的更新方程与（12）到（16）完全相同, 除了预测误差 $e_j$ 被 $EI_j$替代。直接给出更新公式

$$
v_{w_I}^{new}=v_{w_I}^{old}-\eta EH^T\quad(35)
$$

EH是一个N维的向量, 每个元素被定义为

$$
EH_i=\sum_{j=1}^{V}EI_j*w_{ij}^{'}\quad (36)
$$

## 3.优化计算效率

对于所有模型, 每个单词都有两个向量表示：输入向量$v_w$和输出向量$v_w^{'}$。学习输入向量比较快, 但是学习输出向量很费时, 从更新方程（22）和（33）可以看出, 为了更新$v_w^{'}$, 对于训练的每一次, 我们需要遍历每一个单词$w_j$, 计算它们的网络输入$u_j$, 概率预测值$y_j$（或者$y_{c,j}$对于skip-gram）, 它们的预测误差$e_j$（或者$EI_J$对于skip-gram）, 最后用预测误差去更新他们的输出向量$v_j^{'}$。

​ 对每个培训实例的所有词汇进行这样的计算是非常昂贵的, 因此不可能扩大到广泛的词汇或广泛的培训语料库。为了解决这个问题, 直觉是限制每个培训实例必须更新的输出向量的数量。实现这一目标的一个优雅方法是分层softmax;另一种方法是抽样, 这将在下一节讨论。

​ 这两种技巧都只优化输出向量更新的计算。在我们的推导中, 我们关注三个值:（1）E, 新的损失函数；（2）$\frac{\partial E}{\partial v_w^{'}}$, 新的对于输出向量的更新方程；（3）$\frac{\partial E}{\partial h}$, 为更新输入向量而反向传播的预测误差的加权和。

### 3.1 分层softmax（少）

![image-20221011124733813](/static/images/202211/image-20221011124733813.png)

分层softmax中, 白色叶节点代表词汇表中的每个词, 黑色是内部节点。图中高亮路径是root到$w_2$的路径, 长度为4, $n(w,j)$表示从root到单词 w 的路径上的第 j 个节点。

在层次softmax模型中, 单词没有输出向量表示。而每个内部节点（V-1个）有一个输出向量$v_{n(w,j)}^{'}$。

输出的概率为：

$$
p(w=w_O)=\prod_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*v_{n(w,j)}^{'T}h)\quad (37)
$$

其中, $ch(n)$是n节点的左孩子；$v_{n(w,j)}^{'}$是内部节点$n(w,j)$的向量表示（“输出向量”）；h是隐含层的输出值（在skip-gram模型中$h=v_{w_I}$；在CBOW中, $h=\frac{1}{C}\sum_{c=1}^{C}v_{w_c}$）；$[x]$是一个特殊的函数定义为：

$$
[x]=\begin{cases}1 & {if x is true} \\-1 & {otherwise}
\end{cases} \quad(38)
$$

通过一个例子直观理解这个等式。看图4, 假设我们计算$w_2$是输出单词的概率, 我们将这个概率定义为从根节点开始的叶节点结束的随机游走的概率。在每一个内部节点, 我们需要分配向左或者向右走的概率。我们定义在节点n向左走的概率为

$$
p(n,left)=\sigma(v_n^{'T}*h) \quad (39)
$$

这个式子由内部节点的向量表示和隐含层的输出值（随后由输入单词的向量表示决定）决定。显然向右走的概率为

$$
p(n,right)=1-\sigma(v_n^{'T}*h)=\sigma(-v_n^{'T}*h)\quad(40)
$$

在图4 中从根节点到$w_2$的路径, 我们可以计算$w_2$成为输出单词的概率为：

$$
p(w_2=W_O)=p(n(w_2,1),left)*p(n(w_2),left)*P(n(w_2,3),right)=\sigma(v_{n(w_2,1)}^{'T}*h)*\sigma(v_{n(w_2,2)}^{'T}*h)*\sigma(-v_{n(w_2,3)}^{'T}*h)\quad(42)
$$

这就是式子（37）。不难验证,

$$
\sum_{i=1}^{V}p(w_i=w_O)=1\quad(43)
$$

使得分层softmax成为一个所有单词的一个多项分布。

现在推导出内部节点的向量表示的参数更新方程。为了简单, 先看单字上下文模型。

为了简化, 定义下式：

$$
[\centerdot]:=[n(w,j+1)=ch(n(w,j))]\quad(44)\\v_j^{'}:=v_{n_{w,j}}^{'}\quad(45)
$$

在一次训练中, 损失函数被定义为

$$
E=-logp(w=w_O|w_I)=-\sum_{j=1}^{L(w)-1}log\sigma([\centerdot]v_j^{'T}h)\quad(46)
$$

计算关于$v_j^{'}h$的导数

![image-20221011193300064](/static/images/202211/image-20221011193300064.png)

当$[\centerdot]=1$时, $t_{j}=1$, 否则 $t_{j}=0$。

然后我们取E的导数与内单位n(w, j)的向量表示有关, 得到

![image-20221011193521205](/static/images/202211/image-20221011193521205.png)

得到更新方程：

![image-20221011193549705](/static/images/202211/image-20221011193549705.png)

赏识应用于 $j=1,2,...,L(w)-1$。我们认为 $\sigma(v_j^{'T})-t_j$是内部节点 n（w,j）的预测误差。每一个内部节点的任务是预测往右孩子还是左孩子走。$t_{j}=1$意味着真实值是走左孩子；$t_{j}=0$是真实值走右孩子。$\sigma(v_j^{'T})$是预测结果。对于一个训练实例, 如果内部单位的预测非常接近地面真理, 它的向量表示$v_{j}^{'}$会移动得很小；否则$v_{j}^{'}$会移动将通过移动(离h更近或更远5)以适当的方向移动, 以减少本实例的预测误差。这个更新方程可以用于CBOW和skip-gram 模型。当用于skip-gram模型时, 我们需要对输出上下文C中的每个字重复此更新过程。

为了反向传播学习输入→隐藏权重的错误, 我们取E的导数与隐藏层的输出, 得到

![image-20221011194513639](/static/images/202211/image-20221011194513639.png)

上式可以直接替换到（23）中, 去获得CBOW的更新公式。对于skip-gram模型, 我们需要为每个词计算一个EH值在 skip-gram 上下文中, 将 EH 值的总和代入 (35) 以获得更新输入向量的方程。

从更新方程中, 我们可以看到每次训练的计算复杂度每个上下文词的实例从 O(V ) 减少到 O(log(V )), 这是一个很大的改进在速度。 我们仍然有大致相同数量的参数（内部单元的 V -1 个向量与最初的 V 个单词输出向量相比）。

### 负采样（更好）

：：

负采样比softmax更直接, 由于每次要更新的输出向量太多, 只更新其中一部分。

显然输出词应保留在样本中更新, 还需要几个词作为负面的例子。抽样过程需要一个概率分布, 可以任意选择。我们称这种分布为噪声分布, 表示为Pn(w)。一个人可以从经验上确定一个好的分布。

在word2vec中, 作者认为以下简化的培训目标能够产生高质量的文字嵌入, 而不是使用一种形式的负抽样来产生定义良好的后多项分布。

![image-20221011195142003](/static/images/202211/image-20221011195142003.png)

其中wO是输出词(即积极样本)和$v_{w_O}^{'}$是它的输出向量；h是隐藏层的输出值：
